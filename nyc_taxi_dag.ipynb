{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Batch Processing and Data Warehouse via Airflow & PySpark\n",
    "5.1 Medallion Architecture Overview\n",
    "Bronze Layer: Raw data extracted from PostgreSQL for each table.\n",
    "Silver Layer: Data cleaning and transformation using PySpark.\n",
    "Gold Layer: Aggregated data (e.g., average fares by pickup location) in a star schema format.\n",
    "5.2 Airflow DAG Example\n",
    "Below is an example DAG (save as nyc_taxi_dag.py in your Airflow DAGs folder):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #808000; text-decoration-color: #808000; font-weight: bold\">/var/folders/4n/68l65j717mxb8994g_2bc9_00000gn/T/ipykernel_75250/3608515548.py:</span><span style=\"color: #808000; text-decoration-color: #808000; font-weight: bold\">2</span><span style=\"color: #808000; text-decoration-color: #808000; font-weight: bold\"> DeprecationWarning</span><span style=\"color: #808000; text-decoration-color: #808000\">: The `airflow.operators.python_operator.PythonOperator` class is deprecated. Please use `</span><span style=\"color: #808000; text-decoration-color: #808000\">'airflow.operators.python.PythonOperator'</span><span style=\"color: #808000; text-decoration-color: #808000\">`.</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1;33m/var/folders/4n/68l65j717mxb8994g_2bc9_00000gn/T/ipykernel_75250/\u001b[0m\u001b[1;33m3608515548.py\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m2\u001b[0m\u001b[1;33m DeprecationWarning\u001b[0m\u001b[33m: The `airflow.operators.python_operator.PythonOperator` class is deprecated. Please use `\u001b[0m\u001b[33m'airflow.operators.python.PythonOperator'\u001b[0m\u001b[33m`.\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #808000; text-decoration-color: #808000; font-weight: bold\">/var/folders/4n/68l65j717mxb8994g_2bc9_00000gn/T/ipykernel_75250/3608515548.py:</span><span style=\"color: #808000; text-decoration-color: #808000; font-weight: bold\">18</span><span style=\"color: #808000; text-decoration-color: #808000; font-weight: bold\"> RemovedInAirflow3Warning</span><span style=\"color: #808000; text-decoration-color: #808000\">: Param `schedule_interval` is deprecated and will be removed in a future release. Please use `schedule` instead.</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1;33m/var/folders/4n/68l65j717mxb8994g_2bc9_00000gn/T/ipykernel_75250/\u001b[0m\u001b[1;33m3608515548.py\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m18\u001b[0m\u001b[1;33m RemovedInAirflow3Warning\u001b[0m\u001b[33m: Param `schedule_interval` is deprecated and will be removed in a future release. Please use `schedule` instead.\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<Task(PythonOperator): transform_data>"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from airflow import DAG\n",
    "from airflow.operators.python_operator import PythonOperator\n",
    "from datetime import datetime, timedelta\n",
    "import pandas as pd\n",
    "import psycopg2\n",
    "import os\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import to_timestamp, col, when, count, avg, expr\n",
    "\n",
    "# ‚úÖ Airflow DAG Configuration\n",
    "default_args = {\n",
    "    \"owner\": \"airflow\",\n",
    "    \"start_date\": datetime(2024, 7, 1),\n",
    "    \"retries\": 2,\n",
    "    \"retry_delay\": timedelta(minutes=5),\n",
    "}\n",
    "\n",
    "dag = DAG(\"nyc_taxi_batch_pipeline\", default_args=default_args, schedule_interval=\"@daily\")\n",
    "\n",
    "# ‚úÖ Database Connection Details\n",
    "DB_CONFIG = {\n",
    "    \"host\": \"localhost\",\n",
    "    \"port\": 5432,\n",
    "    \"database\": \"nyc_taxi\",\n",
    "    \"user\": \"your_username\",\n",
    "    \"password\": \"your_password\"\n",
    "}\n",
    "\n",
    "TABLES = [\"yellow_tripdata_2024_01\", \"yellow_tripdata_2024_02\", \"green_tripdata_2024_01\", \"green_tripdata_2024_02\"]\n",
    "\n",
    "# ‚úÖ Extract Data and Save as Partitioned CSV\n",
    "def extract_data(**kwargs):\n",
    "    try:\n",
    "        conn = psycopg2.connect(**DB_CONFIG)\n",
    "        os.makedirs(\"data/bronze\", exist_ok=True)\n",
    "\n",
    "        for table in TABLES:\n",
    "            query = f\"SELECT * FROM public.{table};\"\n",
    "            df = pd.read_sql(query, conn)\n",
    "\n",
    "            # Save partitioned by taxi type\n",
    "            taxi_type = \"yellow\" if \"yellow\" in table else \"green\"\n",
    "            month = table[-2:]\n",
    "            df.to_csv(f\"data/bronze/{taxi_type}_tripdata_2024_{month}.csv\", index=False)\n",
    "\n",
    "        conn.close()\n",
    "    except Exception as e:\n",
    "        print(f\"‚ùå Error extracting data: {str(e)}\")\n",
    "        raise\n",
    "\n",
    "# ‚úÖ Transform Data and Apply Feature Engineering in Spark\n",
    "def transform_data(**kwargs):\n",
    "    spark = SparkSession.builder.appName(\"NYC_Taxi_Transformation\").getOrCreate()\n",
    "    \n",
    "    bronze_path = \"data/bronze/\"\n",
    "    silver_path = \"data/silver/\"\n",
    "    gold_path = \"data/gold/\"\n",
    "    os.makedirs(silver_path, exist_ok=True)\n",
    "    os.makedirs(gold_path, exist_ok=True)\n",
    "\n",
    "    for file in os.listdir(bronze_path):\n",
    "        if file.endswith(\".csv\"):\n",
    "            df = spark.read.csv(bronze_path + file, header=True, inferSchema=True)\n",
    "\n",
    "            # ‚úÖ Convert timestamps and clean missing values\n",
    "            df = df.withColumn(\"pickup_datetime\", to_timestamp(col(\"pickup_datetime\"), \"yyyy-MM-dd HH:mm:ss\"))\\\n",
    "                   .na.fill({\"fare_amount\": 0, \"extra\": 0, \"mta_tax\": 0, \"tip_amount\": 0, \"tolls_amount\": 0, \n",
    "                             \"improvement_surcharge\": 0, \"total_amount\": 0})\n",
    "\n",
    "            # ‚úÖ Feature Engineering: Compute Tip & Toll Ratio\n",
    "            df = df.withColumn(\"tip_ratio\", when(col(\"total_amount\") > 0, col(\"tip_amount\") / col(\"total_amount\")).otherwise(0.0))\\\n",
    "                   .withColumn(\"toll_ratio\", when(col(\"total_amount\") > 0, col(\"tolls_amount\") / col(\"total_amount\")).otherwise(0.0))\n",
    "\n",
    "            # ‚úÖ Save Silver Layer (processed data)\n",
    "            df.write.mode(\"overwrite\").parquet(silver_path + file.replace(\".csv\", \"_silver.parquet\"))\n",
    "\n",
    "            # ‚úÖ Gold Layer Aggregations\n",
    "            gold_df = df.groupBy(\"pulocationid\")\\\n",
    "                        .agg(\n",
    "                            count(\"*\").alias(\"total_trips\"),\n",
    "                            avg(\"fare_amount\").alias(\"avg_fare\"),\n",
    "                            expr(\"percentile_approx(fare_amount, 0.5)\").alias(\"median_fare\"),\n",
    "                            expr(\"max(fare_amount)\").alias(\"max_fare\")\n",
    "                        )\n",
    "\n",
    "            gold_df.write.mode(\"overwrite\").parquet(gold_path + file.replace(\".csv\", \"_gold.parquet\"))\n",
    "\n",
    "    spark.stop()\n",
    "\n",
    "# ‚úÖ Define Airflow Tasks\n",
    "t1 = PythonOperator(task_id=\"extract_data\", python_callable=extract_data, dag=dag)\n",
    "t2 = PythonOperator(task_id=\"transform_data\", python_callable=transform_data, dag=dag)\n",
    "\n",
    "# ‚úÖ Set Task Dependencies\n",
    "t1 >> t2\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Interactive Dashboard with Plotly Dash\n",
    "Use Dash to create a dashboard that periodically reads prediction data from Redis. This example works for predictions from any table since each Redis key encodes the source table."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "        <iframe\n",
       "            width=\"100%\"\n",
       "            height=\"650\"\n",
       "            src=\"http://127.0.0.1:8050/\"\n",
       "            frameborder=\"0\"\n",
       "            allowfullscreen\n",
       "            \n",
       "        ></iframe>\n",
       "        "
      ],
      "text/plain": [
       "<IPython.lib.display.IFrame at 0x1275072e0>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import dash\n",
    "from dash import html, dcc\n",
    "from dash.dependencies import Output, Input\n",
    "import redis\n",
    "import json\n",
    "import plotly.express as px\n",
    "import pandas as pd\n",
    "\n",
    "r = redis.StrictRedis(host='localhost', port=6379, decode_responses=True)\n",
    "\n",
    "def get_predictions():\n",
    "    keys = r.keys(\"trip:*\")  # Ensure correct key format\n",
    "    data = []\n",
    "    for key in keys:\n",
    "        record = json.loads(r.get(key))\n",
    "        record[\"key\"] = key\n",
    "        record[\"predicted_fare\"] = record.get(\"fare_prediction\", 0)  # ‚úÖ Ensure this exists\n",
    "        record[\"predicted_duration\"] = record.get(\"duration_prediction\", 0)  # ‚úÖ Include duration prediction\n",
    "        data.append(record)\n",
    "    return data\n",
    "\n",
    "app = dash.Dash(__name__)\n",
    "app.layout = html.Div([\n",
    "    html.H3(\"NYC Taxi Predictions Dashboard\"),\n",
    "    dcc.Interval(id='interval-component', interval=10*1000, n_intervals=0),\n",
    "    dcc.Graph(id='fare-prediction-graph'),\n",
    "    dcc.Graph(id='duration-prediction-graph')\n",
    "])\n",
    "\n",
    "@app.callback(\n",
    "    [Output('fare-prediction-graph', 'figure'),\n",
    "     Output('duration-prediction-graph', 'figure')],\n",
    "    [Input('interval-component', 'n_intervals')]\n",
    ")\n",
    "def update_graph(n):\n",
    "    data = get_predictions()\n",
    "    if data:\n",
    "        df = pd.DataFrame(data)\n",
    "        \n",
    "        # üöñ Plot Fare Predictions\n",
    "        fare_fig = px.bar(df, x=\"key\", y=\"predicted_fare\", title=\"Predicted Fare per Trip\")\n",
    "\n",
    "        # ‚è≥ Plot Duration Predictions\n",
    "        duration_fig = px.bar(df, x=\"key\", y=\"predicted_duration\", title=\"Predicted Trip Duration per Trip\")\n",
    "\n",
    "    else:\n",
    "        fare_fig = px.bar(title=\"No fare predictions available yet\")\n",
    "        duration_fig = px.bar(title=\"No duration predictions available yet\")\n",
    "\n",
    "    return fare_fig, duration_fig\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    app.run_server(debug=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Summary\n",
    "This complete solution now:\n",
    "\n",
    "Ingests data from four PostgreSQL tables (yellow and green for two months each).\n",
    "Simulates real‚Äëtime streaming by publishing rows (with a table identifier) to Kafka.\n",
    "Processes streaming data with Spark Structured Streaming, calls a REST API to get fare predictions, and stores them in Redis.\n",
    "Trains and deploys an XGBoost model served via Flask.\n",
    "Orchestrates batch processing using Airflow with a medallion architecture.\n",
    "Displays results interactively using a Plotly Dash dashboard.\n",
    "All components run with free, open‚Äësource tools and can be developed and executed within a Jupyter Notebook environment. Adjust the code snippets as needed to fit your data schema and deployment environment."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.9 (conda-forge)",
   "language": "python",
   "name": "py39"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
